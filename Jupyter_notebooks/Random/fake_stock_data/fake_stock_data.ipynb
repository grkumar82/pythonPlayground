{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# test stock trades data from Mockaroo, limited to 1k rows\n",
    "# Questions to answer\n",
    "# What is the average stock price for each stock?\n",
    "# which stock had the most trades in a single date (transacted in a single day)?\n",
    "# Which stock saw the most volatililty in total number of stocks traded in a 2-day window?\n",
    "# which stock was the most sold and which one was most bought?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a05eb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2040d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = \"MOCK_DATA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e941d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 22:44:19 WARN Utils: Your hostname, Ravis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.6.23 instead (on interface en0)\n",
      "23/05/15 22:44:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/15 22:44:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"sample_stock_data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfbf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b601ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb0a42b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trade_id: integer (nullable = true)\n",
      " |-- stock_symbol: string (nullable = true)\n",
      " |-- trade_type: string (nullable = true)\n",
      " |-- trade_date: string (nullable = true)\n",
      " |-- trade_time: string (nullable = true)\n",
      " |-- trade_price: double (nullable = true)\n",
      " |-- trade_volume: integer (nullable = true)\n",
      " |-- trade_currency: string (nullable = true)\n",
      " |-- trade_fee: double (nullable = true)\n",
      " |-- trade_notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b8075e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicates(df):\n",
    "    dup_cnt, no_dup_cnt = stocks_df.count(), stocks_df.distinct().count()\n",
    "    diff = dup_cnt - no_dup_cnt\n",
    "    if diff:\n",
    "        return df.dropDuplicates()\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfacef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df = removeDuplicates(stocks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "917b9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that have any NULL values\n",
    "columns = ['trade_id', 'stock_symbol', 'trade_type', 'trade_date', 'trade_time', 'trade_price', 'trade_volume', 'trade_currency', \\\n",
    "           'trade_fee', 'trade_notes']\n",
    "stocks_df = stocks_df.na.drop(subset=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a11789eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df.createOrReplaceTempView(\"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f74f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|stock_symbol|avg_stock_price|\n",
      "+------------+---------------+\n",
      "|        AAPL|       492361.0|\n",
      "|        AMZN|       518753.0|\n",
      "|          FB|       466061.0|\n",
      "|       GOOGL|       498964.0|\n",
      "|        TSLA|       499001.0|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the average stock price for each stock?\n",
    "avg_stock = spark.sql(\n",
    "            f\"\"\"\n",
    "            SELECT \n",
    "            stock_symbol,\n",
    "            ROUND(AVG(trade_price)) AS avg_stock_price\n",
    "            FROM stocks\n",
    "            GROUP BY 1\n",
    "            ORDER BY 1\n",
    "            \"\"\"\n",
    "            )\n",
    "avg_stock.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06b1620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|stock_symbol|avg_stock_price|\n",
      "+------------+---------------+\n",
      "|        AAPL|       492361.0|\n",
      "|        AMZN|       518753.0|\n",
      "|          FB|       466061.0|\n",
      "|       GOOGL|       498964.0|\n",
      "|        TSLA|       499001.0|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "avg_stock2 = stocks_df.groupBy('stock_symbol')\\\n",
    "                        .agg(F.round(F.avg('trade_price')).alias('avg_stock_price'))\\\n",
    "                        .orderBy('stock_symbol')\n",
    "avg_stock2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "288838ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|stock_symbol|\n",
      "+------------+\n",
      "|          FB|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# which stock had the most trades in a single date (transacted in a single day)?\n",
    "most_trades_in_a_day = spark.sql(\n",
    "                    f\"\"\"\n",
    "                    WITH CTE AS \n",
    "                    (\n",
    "                    SELECT \n",
    "                    stock_symbol,\n",
    "                    trade_date,\n",
    "                    COUNT(trade_id) AS trades_cnt\n",
    "                    FROM stocks\n",
    "                    GROUP BY 1, 2\n",
    "                    ORDER BY 3 DESC\n",
    "                    LIMIT 1\n",
    "                    )\n",
    "                    SELECT \n",
    "                    DISTINCT stock_symbol\n",
    "                    FROM CTE\n",
    "                    \"\"\"\n",
    "                    )\n",
    "most_trades_in_a_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b9e25af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|stock_symbol|\n",
      "+------------+\n",
      "|          FB|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_trades_in_a_day2 = stocks_df.groupBy('stock_symbol', 'trade_date')\\\n",
    ".agg(F.count('trade_id').alias('trades_cnt'))\\\n",
    ".orderBy(F.desc('trades_cnt'))\\\n",
    ".limit(1)\\\n",
    ".select('stock_symbol')\n",
    "\n",
    "most_trades_in_a_day2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8785c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|stock_symbol|diff_shares_cnt|\n",
      "+------------+---------------+\n",
      "|        AAPL|        2174607|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which stock saw the most volatililty in total number of stocks traded in a 2-day window?\n",
    "most_volatile = spark.sql(\"\"\"\n",
    "WITH CTE AS \n",
    "(\n",
    "SELECT \n",
    "stock_symbol,\n",
    "trade_date,\n",
    "SUM(trade_volume) as total_shares\n",
    "FROM stocks\n",
    "GROUP BY 1,2\n",
    ")\n",
    "\n",
    "SELECT \n",
    "stock_symbol,\n",
    "diff_shares_cnt\n",
    "FROM \n",
    "(\n",
    "SELECT \n",
    "stock_symbol,\n",
    "trade_date,\n",
    "(total_shares - LAG(total_shares, 1) OVER (PARTITION BY stock_symbol ORDER BY trade_date)) AS diff_shares_cnt\n",
    "FROM CTE\n",
    ") temp\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "most_volatile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93bf597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|stock_symbol|diff_shares_cnt|\n",
      "+------------+---------------+\n",
      "|        AAPL|        2174607|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "cte = stocks_df.groupBy('stock_symbol', 'trade_date')\\\n",
    ".agg(F.sum('trade_volume').alias('total_shares'))\n",
    "\n",
    "window_spec = Window.partitionBy('stock_symbol').orderBy('trade_date')\n",
    "diff_shares = cte.select('stock_symbol', 'trade_date', \n",
    "    (F.col('total_shares') - F.lag('total_shares').over(window_spec)).alias('diff_shares_cnt'))\n",
    "result = diff_shares.orderBy(F.desc('diff_shares_cnt')).limit(1)\\\n",
    ".select('stock_symbol', 'diff_shares_cnt')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81a8ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|stock_symbol|description|\n",
      "+------------+-----------+\n",
      "|        AAPL|  most_sold|\n",
      "|          FB|most_bought|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# which stock was the most sold and which one was most bought?\n",
    "most_sold_bought = spark.sql(\n",
    "f\"\"\"\n",
    "WITH trade_cnt AS \n",
    "(\n",
    "SELECT \n",
    "stock_symbol,\n",
    "SUM(CASE WHEN trade_type = 'buy' THEN 1 ELSE 0 END) as bought,\n",
    "SUM(CASE WHEN trade_type = 'sold' THEN 1 ELSE 0 END) as sold\n",
    "FROM stocks\n",
    "GROUP BY 1\n",
    "),\n",
    "most_sold AS \n",
    "(\n",
    "SELECT stock_symbol, 'most_sold' AS description\n",
    "FROM trade_cnt\n",
    "ORDER BY sold DESC\n",
    "LIMIT 1\n",
    "),\n",
    "most_bought AS \n",
    "(\n",
    "SELECT stock_symbol, 'most_bought' AS description\n",
    "FROM trade_cnt\n",
    "ORDER BY bought DESC\n",
    "LIMIT 1\n",
    ")\n",
    "SELECT * FROM most_sold\n",
    "UNION ALL\n",
    "SELECT * FROM most_bought\n",
    "\"\"\"                \n",
    ")\n",
    "most_sold_bought.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c78565fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|stock_symbol|description|\n",
      "+------------+-----------+\n",
      "|        AAPL|  most_sold|\n",
      "|          FB|most_bought|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "trade_cnt = (\n",
    "stocks_df.groupBy(\"stock_symbol\")\n",
    "    .agg(sum(when(stocks_df['trade_type'] == 'buy', 1).otherwise(0)).alias(\"bought\"),\n",
    "    sum(when(stocks_df['trade_type'] == 'sold', 1).otherwise(0)).alias(\"sold\"))\n",
    ")\n",
    "\n",
    "most_sold = (\n",
    "trade_cnt\n",
    "    .orderBy(\"sold\", ascending=False)\n",
    "    .limit(1)\n",
    "    .select(\"stock_symbol\", lit(\"most_sold\").alias(\"description\"))\n",
    ")\n",
    "most_bought = (\n",
    "trade_cnt\n",
    "    .orderBy(\"bought\", ascending=False)\n",
    "    .limit(1)\n",
    "    .select(\"stock_symbol\", lit(\"most_bought\").alias(\"description\"))\n",
    ")\n",
    "\n",
    "most_sold_bought = most_sold.union(most_bought)\n",
    "most_sold_bought.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86f106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
